<!DOCTYPE html>
<html lang="en">
<head>
  <title>EmoStyle</title>
  <meta name="description" content="EmoStyle: One-Shot Facial Expression Editing Using Continuous Emotion Parameters.">
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes">
  <meta charset="utf-8">

  <!--Facebook preview-->
  <!-- <meta property="og:image" content="https://imagine.enpc.fr/~monniert/DTIClustering/thumbnail.png">
  <meta property="og:image:type" content="image/png">
  <meta property="og:image:width" content="600">
  <meta property="og:image:height" content="400">
  <meta property="og:type" content="website"/>
  <meta property="og:url" content="https://imagine.enpc.fr/~monniert/DTIClustering/"/>
  <meta property="og:title" content="DTI Clustering"/>
  <meta property="og:description" content="Project page for Deep Transformation-Invariant Clustering."/> -->

  <!--Twitter preview-->
  <!-- <meta name="twitter:card" content="summary_large_image" /> -->
  <!-- <meta name="twitter:title" content="DTI Clustering" /> -->
  <!-- <meta name="twitter:description" content="Project page for Deep Transformation-Invariant Clustering."/> -->
  <!-- <meta name="twitter:image" content=""> -->
  <!--Style-->
  <!-- <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link href="style.css" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script> -->

</head>
<body>

<div class="container" style="text-align:center; padding:2rem 15px">
  <div class="row" style="text-align:center">
    <h1>EmoStyle: One-Shot Facial Expression Editing Using Continuous Emotion Parameters</h1>
    <!-- <h4>NeurIPS 2020 (oral presentation)</h4> -->
  </div>
  <div class="row" style="text-align:center">
    <div class="col-xs-0 col-md-3"></div>
    <div class="col-xs-12 col-md-6">
      <h4>
        <a><nobr>Bita Azari*</nobr></a> &emsp;
        <a><nobr>Angelica Lim*</nobr></a> &emsp;
      </h4>
      *Simon Fraser University, <nobr>Burnaby</nobr>, <nobr>BC</nobr>, Canada
    </div>
    <!-- <div class="hidden-xs hidden-sm col-md-1" style="text-align:left; margin-left:0px; margin-right:0px">
      <a href="https://arxiv.org/pdf/2006.11132.pdf" style="color:inherit">
        <i class="fa fa-file-pdf-o fa-4x"></i></a> 
    </div>
    <div class="hidden-xs hidden-sm col-md-2" style="text-align:left; margin-left:0px;">
      <a href="https://github.com/monniert/dti-clustering" style="color:inherit">
        <i class="fa fa-github fa-4x"></i></a>
    </div> -->
  </div>
</div>

<div class="container" style="text-align:center; padding:1rem">
  <img src="resrc/samples.png" alt="samples.png" class="text-center" style="width: 100%; max-width: 1100px">
  <h3 style="text-align:center; padding-top:1rem">
    <a class="label label-info" href="https://openaccess.thecvf.com/content/WACV2024/papers/Azari_EmoStyle_One-Shot_Facial_Expression_Editing_Using_Continuous_Emotion_Parameters_WACV_2024_paper.pdf">Paper</a>
    <a class="label label-info" href="">Code<span style="font-size: smaller; font-weight: normal;"> (coming soon)</span></a>
    <!-- <a class="label label-info" href="https://www.youtube.com/embed/j20MBc1hWGQ">Video</a>
    <a class="label label-info" href="resrc/dtic_long.pptx">Slides</a>
    <a class="label label-info" href="resrc/ref.bib">BibTeX</a> -->
  </h3>
</div>

<div class="container">
  <h3>Abstract</h3>
  <hr/>
  <p>
    Recent studies have achieved impressive results in face generation and editing of facial expressions.
    However, existing approaches either generate a discrete number of facial expressions or have limited control over the emotion of the output image.
    To overcome this limitation, we introduced EmoStyle, a method to edit facial expressions based on valence and arousal, two continuous emotional parameters that can specify a broad range of emotions.
    EmoStyle is designed to separate emotions from other facial characteristics and to edit the face to display a desired emotion.
    We employ the pre-trained generator from StyleGAN2, taking advantage of its rich latent space.
    We also proposed an adapted inversion method to be able to apply our system on out-of-StyleGAN2 domain (OOD) images in a one-shot manner.
    The qualitative and quantitative evaluations show that our approach has the capability to synthesize a wide range of expressions to output high-resolution images.
  </p>

  <!-- <h3>Video</h3>
  <hr/>
  <div class="row" style="text-align:center">
    <div class="col-xs-6 text-center">
      <h4><u>Short presentation</u> (3min)</h4>
      <div class="embed-responsive embed-responsive-16by9" style="text-align:center">
        <iframe class="embed-responsive-item text-center" src="https://www.youtube.com/embed/j20MBc1hWGQ" frameborder="0"
          allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
          style="width:100%; clip-path:inset(1px 1px);" allowfullscreen></iframe>
      </div>
    </div>

    <div class="col-xs-6 text-center">
      <h4><u>Long presentation</u> (11min)</h4>
      <div class="embed-responsive embed-responsive-16by9" style="text-align:center">
        <iframe class="embed-responsive-item text-center" src="https://www.youtube.com/embed/xhLUOh5PKBA" frameborder="0"
          allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
          style="width:100%; clip-path:inset(1px 1px);" allowfullscreen></iframe>
      </div>
    </div>
  </div> -->

  <!-- <h3>Approach</h3>
  <hr/>
  <div class="row" style="text-align: center">
    <div class="col-xs-6">
      <h4 style="margin-right: 20%"><u>DTI framework</u></h4>
    </div>
    <div class="col-xs-6">
      <h4><u>Deep transformation module 
          <img src="http://latex.codecogs.com/svg.latex?\mathcal{T}_{f_{k}}" alt="T_f_k" border="0"/></u></h4>
    </div>
  </div>
  <div class="row" style="text-align: center">
    <div class="col-xs-6">
      <img src="resrc/dti.png" alt="dti.png" class="text-center" style="width: 100%; max-width: 900px">
    </div>
    <div class="col-xs-6">
      <img src="resrc/deep_tsf.png" alt="deep_tsf.png" class="text-center" style="width: 90%; max-width: 900px; margin-top:
      10px">
    </div>
  </div>
  <div class="row" style="text-align: center">
    <div class="col-xs-6">
      <div style="width: 90%; max-width: 900px; padding-top:10px">
      <p>Given a sample <img src="http://latex.codecogs.com/svg.latex?x_i" alt="x_i" border="0"/> and prototypes 
      <img src="http://latex.codecogs.com/svg.latex?c_1" alt="c_1" border="0"/> and 
      <img src="http://latex.codecogs.com/svg.latex?c_2" alt="c_2" border="0"/>, standard clustering such as K-means 
      assigns the sample to the closest prototype. Our DTI clustering first aligns prototypes to the sample using a
      family of parametric transformations - here rotations - then picks the prototype whose alignment yields the 
      smallest distance.</p>
      </div>
    </div>
    <div class="col-xs-6">
      <div style="width: 100%; max-width: 900px; padding-top:10px">
        <p>We predict alignment with deep learning. Given an image 
      <img src="http://latex.codecogs.com/svg.latex?x_i" alt="x_i" border="0"/>, each deep parameter predictor 
      <img src="http://latex.codecogs.com/svg.latex?f_k" alt="f_k" border="0"/> predicts 
      parameters for a sequence of transformations - here affine, morphological and thin plate spline transformations -
      to align the prototype <img src="http://latex.codecogs.com/svg.latex?c_k" alt="c_k" border="0"/>
      to the query image <img src="http://latex.codecogs.com/svg.latex?x_i" alt="x_i" border="0"/>.</p>
      </div>
    </div>
  </div> -->


  <h3>Approach</h3>
  <hr/>
  <div class="row" style="text-align: center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
      <img src="resrc/workflow.png" alt="workflow.png" class="text-center" style="width: 100%; max-width: 1000px;">
  </div>
  <div class="row" style="text-align:center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
      <p><strong>Phase 1 - Training EmoExtract:</strong> We train the <strong>EmoExtract</strong> and up-sampling modules (green) by alternating Emotion Variation with random emotion parameters from the valence-arousal space (top),
        with Emotion Reconstruction of the input face (bottom). Five auxiliary losses are used for this purpose, as indicated by the dashed lines.
        The Inversion module is employed to extract the latent code <i>w</i> of the input image <i>I<sub>input</sub></i>.
        The <strong>EmoExtract</strong> module is trained to determine the necessary modifications <i>d</i> that should be applied to a latent code <i>w</i>.
        Note that <i>d</i> should result in 0 for the Emotion Reconstruction segment.
        The final latent code is generated by adding <i>d</i> to the original latent code <i>w</i>.
        Finally, the StyleGAN2 generator is used to create our desired image.</p>
      <p><strong>Phase 2 - Fine-tuning StyleGAN2:</strong> we freeze the <strong>EmoExtract</strong> module trained previously and fine-tune our StyleGAN2 component.
        Our inputs during this phase are emotion parameters and one out-of-domain face.
        First, we determine the face's latent code utilizing an inversion framework to extract the latent code in the StyleGAN2 <i>W</i> space,
        then perform a fine-tuning step.
        <!-- step inspired from <cite>roich2022pivotal</cite>. -->
      </p>
  </div>

  <h3>Results</h3>
  <hr/>
  <div class="row" style="text-align: center; padding-left:1rem; padding-right:1rem; padding-bottom:1rem;">
    <h4><u>Results on StyleGAN images</u></h4>
      <img src="resrc/more.png" alt="results.ng" class="text-center" style="width: 100%; max-width: 1000px;">
  </div>
  <div style="text-align: center; padding-left: 1rem; padding-right: 1rem; padding-bottom: 1rem;">
    <h4><u>Results on real images</u></h4>
    <div style="display: inline-block; width: calc(50% - 10px); box-sizing: border-box; padding-right: 10px;">
      <img src="resrc/barak_obama.gif" alt="barak_obama.gif" style="width: 100%;">
    </div>
    <div style="display: inline-block; width: calc(50% - 10px); box-sizing: border-box; padding-left: 10px;">
      <img src="resrc/taylor_swift.gif" alt="taylor_swift.gif" style="width: 100%;">
    </div>
  </div>
  

  <!-- <h3>Resources</h3>
  <hr/>
    <h4 style="padding-top:0.5em">BibTeX</h4>
    If you find this work useful for your research, please cite:
    <div class="card">
      <div class="card-block">
        <pre class="card-text clickselect">
    @unpublished{AzariBita2023,
      author = {Bita Azari, Angelica Lim},
      title = {EmoStyle: One-Shot Facial Expression Editing Using Continuous Emotion Parameters},
      note = {Unpublished work},
      year = {2023}
    }
      </div>
    </div> -->

  <!-- <h3>Further information</h3>
  <hr/>
  If you like this project, please check out other related works from our group:
  <h4>Follow-ups</h4>
  <ul>
    <li>
      <a href="https://arxiv.org/abs/2104.14575">Monnier et al. - Unsupervised Layered Image Decomposition into Object
        Prototypes (arXiv 2021)</a>
    </li>
  </ul>

  <h4>Previous works on deep transformations</h4>
  <ul>
    <li>
      <a href="https://arxiv.org/abs/1908.04725">Deprelle et al. - Learning elementary structures for 3D shape
        generation and matching (NeurIPS 2019)</a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/1806.05228">Groueix et al. - 3D-CODED: 3D Correspondences by Deep Deformation (ECCV
        2018)</a>
    </li>
    <li>
      <a href="https://arxiv.org/abs/1802.05384">Groueix et al. - AtlasNet: A Papier-Mache Approach to Learning 3D
        Surface Generation (CVPR 2018)</a>
    </li>
  </ul> -->


  <!-- <h3>Acknowledgements</h3>
  <hr/>
  <p>
    This work was supported in part by <a href="https://enherit.enpc.fr/">ANR project EnHerit</a> ANR-17-CE23-0008,
    project Rapid Tabasco, gifts  from  Adobe and HPC resources from GENCI-IDRIS (Grant 2020-AD011011697). We thank 
    Bryan Russell, Vladimir Kim, Matthew Fisher, Fran&#231;ois Darmon, Simon Roburin, David Picard, Michael 
    Ramamonjisoa, Vincent Lepetit, Elliot Vincent, Jean Ponce, William Peebles and Alexei Efros for inspiring 
    discussions and valuable feedback.
  </p>
</div> -->

<div class="container" style="padding-top:3rem; padding-bottom:3rem">
  <p style="text-align:center">
  &#169; This webpage was in part inspired from this
  <a href="https://github.com/monniert/project-webpage">template</a>.
  </p>
</div>

</body>
</html> 
